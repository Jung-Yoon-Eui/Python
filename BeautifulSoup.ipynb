{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑 연습\n",
      "p1 = 웹 페이지 분석해보기\n",
      "p2 = 데이터 정제하기\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>스크레이핑 연습</h1>\n",
    "        <p>웹 페이지 분석해보기</p>\n",
    "        <p>데이터 정제하기</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "                            # parser : 읽어오기\n",
    "    \n",
    "# 원하는 요소 접근하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "        # next_sibling : 지금 p태그가 2개 있음. 기본적으로 가장 첫번째 것 인식.\n",
    "        #      다음 차례에 있는 동일 태그의 내용을 읽기 위해 next_sibling 사용.\n",
    "\n",
    "# 원하는 요소의 내용을 추출하기\n",
    "print(\"h1 = \"+ h1.string)\n",
    "print(\"p1 = \"+ p1.string)\n",
    "print(\"p2 = \"+ p2.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title = BeautifulSoup 사용법 연습\n",
      "subTitle = 스크레이핑 연습\n",
      "p = 원하는 데이터 추출하기\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup 사용법 : find() 메소드를 이용한 데이터 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id=\"title\">BeautifulSoup 사용법 연습</h1>\n",
    "        <p id=\"subTitle\">스크레이핑 연습</p>\n",
    "        <p>원하는 데이터 추출하기</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "                            # parser : 읽어오기\n",
    "    \n",
    "# find() 메소드를 이용한 데이터 추출하기\n",
    "#- 하나의 요소 값을 읽어옴.\n",
    "#- html id 속성 값을 가지고 올 수 있음.\n",
    "title = soup.find(id=\"title\")\n",
    "subTitle = soup.find(id=\"subTitle\")\n",
    "p = subTitle.next_sibling.next_sibling\n",
    "\n",
    "# 출력\n",
    "print(\"title = \"+ title.string)\n",
    "print(\"subTitle = \"+ subTitle.string)\n",
    "print(\"p = \"+ p.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이버 > https://www.naver.com\n",
      "다음 > https://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup 사용법 : find_all() 메소드를 이용한 데이터 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul>\n",
    "            <li><a href=\"https://www.naver.com\">네이버</a><li>\n",
    "            <li><a href=\"https://www.daum.net\">다음</a><li>\n",
    "        <ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "                            # parser : 읽어오기\n",
    "    \n",
    "# find_all() 메소드를 이용한 데이터 추출하기\n",
    "#- 전체 데이터 읽어옴.\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# 출력\n",
    "for a in links:\n",
    "    href = a.attrs['href']  # attrs : 속성\n",
    "    text = a.string\n",
    "    print(text, \">\" ,href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url =  https://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108\n",
      "title = 전국 육상중기예보\n",
      "wf = ○ (강수) 7일(토)은 서울.경기도와 강원영서, 충청도, 전북에 비가 오겠습니다.<br />○ (기온) 이번 예보기간 아침 기온은 -2~14도로 어제(3~10도)와 비슷하거나 조금 낮겠고, 낮 기온은 10~22도로 어제(7~13도)보다 높겠습니다.<br />          한편, 8일(일)~9일(월) 아침 기온은 -2~9도로 춥겠고, 낮 기온도 10~17도로 낮겠습니다.  <br />○ (건조) 이번 예보기간 동안 대체로 맑은 날씨가 이어지면서 대기가 건조하겠으니, 산불 등 화재 예방에 유의하기 바랍니다.<br />○ (주말전망) 7일(토)은 전국이 가끔 구름많고 오전에 서울.경기도와 강원영서, 충청도, 전북에 비가 오겠고, 8일(일)은 전국이 맑겠습니다. <br />              아침 기온은 0~14도, 낮 기온은 10~22도의 분포가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.parse  # url 인코딩\n",
    "\n",
    "rssUrl = \"https://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "values = {\n",
    "    'stnId':108\n",
    "}\n",
    "\n",
    "params = urllib.parse.urlencode(values)\n",
    "\n",
    "url = rssUrl + \"?\" + params\n",
    "\n",
    "print(\"url = \",url)\n",
    "\n",
    "data = urllib.request.urlopen(url).read()\n",
    "text = data.decode(\"utf-8\")\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(text,'html.parser')\n",
    "                            # parser : 읽어오기\n",
    "\n",
    "header = soup.rss.channel.item.description.header\n",
    "title = header.title\n",
    "wf = header.wf\n",
    "print(\"title = \"+ title.string)\n",
    "print(\"wf = \"+ wf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 7일(토)은 서울.경기도와 강원영서, 충청도, 전북에 비가 오겠습니다.<br />○ (기온) 이번 예보기간 아침 기온은 -2~14도로 어제(3~10도)와 비슷하거나 조금 낮겠고, 낮 기온은 10~22도로 어제(7~13도)보다 높겠습니다.<br />          한편, 8일(일)~9일(월) 아침 기온은 -2~9도로 춥겠고, 낮 기온도 10~17도로 낮겠습니다.  <br />○ (건조) 이번 예보기간 동안 대체로 맑은 날씨가 이어지면서 대기가 건조하겠으니, 산불 등 화재 예방에 유의하기 바랍니다.<br />○ (주말전망) 7일(토)은 전국이 가끔 구름많고 오전에 서울.경기도와 강원영서, 충청도, 전북에 비가 오겠고, 8일(일)은 전국이 맑겠습니다. <br />              아침 기온은 0~14도, 낮 기온은 10~22도의 분포가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import urllib.parse  # url 인코딩\n",
    "\n",
    "res = req.urlopen(rssUrl)\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "                            # parser : 읽어오기\n",
    "    \n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빅데이터 분석 강좌\n",
      "li = R언어 강좌\n",
      "li = 머신러닝을 위한 데이터처리\n",
      "li = 파이썬으로 익히는 딥러닝이론\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석 대상 html문서\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <div id=\"LecList\">\n",
    "        <h1>데이터 과학<h1>\n",
    "    </div>\n",
    "    <div id=\"lecture\">\n",
    "        <h1>빅데이터 분석 강좌</h1>\n",
    "        <ul class=\"subject\">\n",
    "            <li>R언어 강좌</li>\n",
    "            <li>머신러닝을 위한 데이터처리</li>\n",
    "            <li>파이썬으로 익히는 딥러닝이론</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# CSS 쿼리로 데이터 추출하기\n",
    "h1 = soup.select_one(\"div#lecture > h1\").string\n",
    "print(h1)                        # > : 하위 계층으로\n",
    "\n",
    "subject = soup.select(\"div#lecture > ul.subject > li\")\n",
    "for li in subject:\n",
    "    print(\"li = \"+ li.string)\n",
    "\n",
    "#css 선택자를 이용한 스크레이핑\n",
    "#   Id : #으로 표현.\n",
    "#   class : .으로 표현.\n",
    "#   select_one() : css선택자로 요소 하나의 선택자로 요소 하나를 추출.\n",
    "#   select() : css선택자로 요소 여러개를 리스트로 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--find()\n",
      "1 R 언어\n",
      "2 R 언어\n",
      "--find_all()\n",
      "3 R 언어\n",
      "4 R 언어\n",
      "5 R 언어\n",
      "6 R 언어\n",
      "--select_one()\n",
      "7 R 언어\n",
      "8 R 언어\n",
      "9 R 언어\n",
      "10 R 언어\n",
      "11 R 언어\n",
      "12 R 언어\n",
      "--select()\n",
      "13 R 언어\n",
      "14 R 언어\n",
      "15 R 언어\n",
      "16 R 언어\n",
      "17 R 언어\n",
      "18 R 언어\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "str=\"\"\"\n",
    "<ul id=\"itBook\">\n",
    "    <li id=\"web\">Spring</li>\n",
    "    <li id=\"Mobile\">Android</li>\n",
    "    <li id=\"DataScience\">R 언어</li>\n",
    "    <li id=\"Database\">Oracle</li>\n",
    "    <li id=\"OS\">Linux</li>\n",
    "    <li id=\"JavaScript\">ReactJs</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(str,'html.parser')\n",
    "\n",
    "print(\"--find()\")\n",
    "# 1\n",
    "DataScience = soup.find(\"li\").next_sibling.next_sibling.next_sibling.next_sibling.string\n",
    "print(1,DataScience)\n",
    "\n",
    "# 2\n",
    "print(2,soup.find(id=\"DataScience\").string)\n",
    "\n",
    "print(\"--find_all()\")\n",
    "# 3\n",
    "DataScience = soup.find_all(\"li\")\n",
    "print(3,DataScience[2].string)\n",
    "\n",
    "# 4\n",
    "DataScience = soup.find_all(\"li\")\n",
    "\n",
    "for a in DataScience:\n",
    "    li = a.attrs['id']\n",
    "    text = a.string\n",
    "    if li==\"DataScience\":\n",
    "        print(4,text)\n",
    "        \n",
    "# 5\n",
    "DataScience = soup.find_all(\"li\")\n",
    "\n",
    "for a in DataScience:\n",
    "    text = a.string\n",
    "    if text==\"R 언어\":\n",
    "        break\n",
    "print(5,text)\n",
    "\n",
    "# 6\n",
    "print(6, soup.find_all(id = \"DataScience\")[0].string)\n",
    "print(6, soup.find_all(string = \"R 언어\")[0].string)\n",
    "\n",
    "print(\"--select_one()\")\n",
    "# 7\n",
    "print(7,soup.ul.select_one(\"li#DataScience\").string)\n",
    "# 8\n",
    "print(8,soup.ul.select_one(\"#DataScience\").string)\n",
    "# 9\n",
    "print(9,soup.select_one(\"ul#itBook > li#DataScience\").string)\n",
    "# 10\n",
    "print(10,soup.select_one(\"ul#itBook li#DataScience\").string)\n",
    "# 11\n",
    "print(11,soup.select_one(\"#itBook > #DataScience\").string)\n",
    "# 12\n",
    "print(12,soup.select_one(\"#itBook #DataScience\").string)\n",
    "\n",
    "print(\"--select()\")\n",
    "# 13\n",
    "DataScience = soup.select(\"ul#itBook > li#DataScience\")\n",
    "for li in DataScience:\n",
    "    print(13,li.string)\n",
    "\n",
    "# 14\n",
    "DataScience = soup.select(\"ul#itBook li#DataScience\")\n",
    "for li in DataScience:\n",
    "    print(14,li.string)\n",
    "    \n",
    "# 15\n",
    "DataScience = soup.select(\"#itBook > #DataScience\")\n",
    "for li in DataScience:\n",
    "    print(15,li.string)\n",
    "    \n",
    "# 16\n",
    "DataScience = soup.select(\"#itBook #DataScience\")\n",
    "for li in DataScience:\n",
    "    print(16,li.string)\n",
    "    \n",
    "# 17\n",
    "DataScience = soup.ul.select(\"li#DataScience\")\n",
    "for li in DataScience:\n",
    "    print(17,li.string)\n",
    "    \n",
    "# 18\n",
    "DataScience = soup.ul.select(\"#DataScience\")\n",
    "for li in DataScience:\n",
    "    print(18,li.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
